{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8f57fa1-ea2a-4636-8049-fe973471f6d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\anaconda3\\envs\\sql_expert\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer, SFTConfig, DataCollatorForCompletionOnlyLM\n",
    "from transformers import EarlyStoppingCallback\n",
    "from transformers.trainer_callback import TrainerCallback\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from unsloth import is_bfloat16_supported\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9cec988-6f9c-481e-bce3-8ee6da1d7065",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "login(os.getenv(\"HF_TOKEN_WRITE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdb53e9e-280e-4fe1-bfb2-dde4c2344391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "# Model\n",
    "BASE_MODEL = \"unsloth/Qwen2.5-7B-instruct\"\n",
    "MAX_SEQ_LENGTH = 4096\n",
    "DTYPE = torch.bfloat16\n",
    "LOAD_IN_4BIT = True\n",
    "\n",
    "# Project\n",
    "HF_USER = \"Yihim\"\n",
    "PROJECT_NAME = \"sql_expert\"\n",
    "RUN_NAME = f\"{datetime.now():%Y-%m-%d_%H.%M.%S}\"\n",
    "PROJECT_RUN_NAME = f\"{PROJECT_NAME}--{RUN_NAME}\"\n",
    "\n",
    "# LoRA\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "LORA_DROPOUT = 0.1\n",
    "\n",
    "# Training\n",
    "MAX_STEPS = 500\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "EVAL_BATCH_SIZE = 2\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "LEARNING_RATE = 2e-5\n",
    "LR_SCHEDULER_TYPE = \"cosine\"\n",
    "WARMUP_RATIO = 0.1\n",
    "WARMUP_STEPS = 500\n",
    "LOG_STEPS = 1\n",
    "SAVE_STEPS = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e83ecd1d-33bc-4f1f-b36a-7c2a76ab707f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\anaconda3\\envs\\sql_expert\\Lib\\site-packages\\unsloth_zoo\\gradient_checkpointing.py:330: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  GPU_BUFFERS = tuple([torch.empty(2*256*2048, dtype = dtype, device = f\"cuda:{i}\") for i in range(n_gpus)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.8: Fast Qwen2 patching. Transformers: 4.49.0. vLLM: 0.7.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4080 SUPER. Num GPUs = 1. Max memory: 15.992 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:08<00:00,  4.30s/it]\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=BASE_MODEL,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=DTYPE,\n",
    "    load_in_4bit=LOAD_IN_4BIT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ef068fd-fb07-43b8-8058-a8ca62672e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.075802112"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_memory_footprint() / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ed24f4b-2d48-4b68-a8ca-d485b83a91a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(152064, 3584, padding_idx=151654)\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
       "          (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "          (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=18944, out_features=3584, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "      )\n",
       "      (2-4): 3 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "      )\n",
       "      (5-24): 20 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=18944, out_features=3584, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "      )\n",
       "      (25-26): 2 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "      )\n",
       "      (27): Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=18944, out_features=3584, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "902777f7-8940-4404-972e-d228cf8b6975",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a0b685b-4c31-4d45-b150-11b71a90bde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.1.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2025.3.8 patched 28 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=LORA_R,\n",
    "    target_modules=TARGET_MODULES,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1817e74e-f372-4e17-8926-808a13d0f848",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: yihimchan (yihimchan-personal) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\PC\\Documents\\self_learning\\0_projects\\sql_expert\\notebooks\\wandb\\run-20250312_112320-107pbsgy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yihimchan-personal/sql_expert/runs/107pbsgy' target=\"_blank\">2025-03-12_11.23.04</a></strong> to <a href='https://wandb.ai/yihimchan-personal/sql_expert' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yihimchan-personal/sql_expert' target=\"_blank\">https://wandb.ai/yihimchan-personal/sql_expert</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yihimchan-personal/sql_expert/runs/107pbsgy' target=\"_blank\">https://wandb.ai/yihimchan-personal/sql_expert/runs/107pbsgy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/yihimchan-personal/sql_expert/runs/107pbsgy?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1902ecabdd0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=PROJECT_NAME, name=RUN_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62ddab5d-3700-4722-851f-af84cbab78eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_dataset = load_dataset(\"gretelai/synthetic_text_to_sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad326f28-36e5-4e1e-9b13-34bd71d7865e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'domain', 'domain_description', 'sql_complexity', 'sql_complexity_description', 'sql_task_type', 'sql_task_type_description', 'sql_prompt', 'sql_context', 'sql', 'sql_explanation'],\n",
       "        num_rows: 100000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'domain', 'domain_description', 'sql_complexity', 'sql_complexity_description', 'sql_task_type', 'sql_task_type_description', 'sql_prompt', 'sql_context', 'sql', 'sql_explanation'],\n",
       "        num_rows: 5851\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77ebe3dd-8438-4d94-aa40-303ada2a1268",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = sql_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1469995a-5556-4103-a418-88b3dd961bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.95 * len(sql_dataset[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6632cce-a30c-4257-805e-a955a51fd570",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.permutation(len(sql_dataset[\"train\"]))\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82a106d3-61ae-423f-b507-effadb50104f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = sql_dataset[\"train\"].select(train_indices)\n",
    "val = sql_dataset[\"train\"].select(val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a131758-772d-4768-97c3-73aef1d7c75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUCTION = \"\"\"You are a specialized SQL query generator that helps users write efficient SQL queries. \n",
    "Your role is to analyze the database schema in the `sql_context` and generate the appropriate SQL code with explanation that answers the `sql_prompt`.\n",
    "Both `sql_context` and `sql_prompt` are given by the user.\n",
    "\n",
    "### Input Example:\n",
    "sql_context: CREATE TABLE salesperson (salesperson_id INT, name TEXT, region TEXT); INSERT INTO salesperson (salesperson_id, name, region) VALUES (1, 'John Doe', 'North'), (2, 'Jane Smith', 'South'); CREATE TABLE timber_sales (sales_id INT, salesperson_id INT, volume REAL, sale_date DATE); INSERT INTO timber_sales (sales_id, salesperson_id, volume, sale_date) VALUES (1, 1, 120, '2021-01-01'), (2, 1, 150, '2021-02-01'), (3, 2, 180, '2021-01-01');\n",
    "\n",
    "sql_prompt: \"What is the total volume of timber sold by each salesperson, sorted by salesperson?\"\n",
    "\n",
    "### Output Example:\n",
    "SQL: SELECT salesperson_id, name, SUM(volume) as total_volume FROM timber_sales JOIN salesperson ON timber_sales.salesperson_id = salesperson.salesperson_id GROUP BY salesperson_id, name ORDER BY total_volume DESC;\n",
    "\n",
    "Explanation: Joins timber_sales and salesperson tables, groups sales by salesperson, calculates total volume sold by each salesperson, and orders the results by total volume in descending order.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "495b1745-9320-46ba-a15a-5bfe551f88c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_EXAMPLE = \"\"\"sql_context: {sql_context}\n",
    "\n",
    "sql_prompt: {sql_prompt}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8cc0c1f-5fe0-4f2e-abfc-056e96f4ae7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_EXAMPLE = \"\"\"SQL: {sql}\n",
    "\n",
    "Explanation: {sql_explanation}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf15fbf3-b691-4b88-ac7c-ea66e3909e6c",
   "metadata": {},
   "source": [
    "## Use alpaca format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8ac882f-0ead-4dec-b9d4-df6c988f4735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALPACA_PROMPT = \"\"\"Below is an instruction that describes a task, paired with an input that provides futther contenxt. Write a response that appropriately completes the request.\n",
    "\n",
    "# ### Instruction:\n",
    "# {instruction}\n",
    "\n",
    "# ### Input:\n",
    "# {input_example}\n",
    "\n",
    "# ### Response:\n",
    "# {output_example}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24ad997c-b3fd-43e3-b511-27d0a4914e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "# def format_example(examples):\n",
    "#     sql_contexts = examples[\"sql_context\"]\n",
    "#     sql_prompts = examples[\"sql_prompt\"]\n",
    "#     sqls = examples[\"sql\"]\n",
    "#     sql_explanations = examples[\"sql_explanation\"]\n",
    "#     texts = []\n",
    "#     for sql_context, sql_prompt, sql, sql_explanation in zip(sql_contexts, sql_prompts, sqls, sql_explanations):\n",
    "#         instruction = INSTRUCTION\n",
    "#         input_example = INPUT_EXAMPLE.format(sql_context=sql_context, sql_prompt=sql_prompt)\n",
    "#         output_example = OUTPUT_EXAMPLE.format(sql=sql, sql_explanation=sql_explanation)\n",
    "#         text = ALPACA_PROMPT.format(instruction=instruction, input_example=input_example, output_example=output_example) + EOS_TOKEN\n",
    "#         texts.append(text)\n",
    "#     return {\"text\": texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e10381cf-e9be-47e1-b839-af84b361f706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = train.map(format_example, batched=True)\n",
    "# val = val.map(format_example, batched=True)\n",
    "# test = test.map(format_example, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b2d5227-2a7f-4874-8c9a-fc1beffe2300",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(val[\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c032006a-f372-47ef-9f65-c698963d9661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response_template = \"### Response:\\n\"\n",
    "# collator = DataCollatorForCompletionOnlyLM(response_template=response_template, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec23c368-701b-436f-b61d-1e7d7250255c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example(examples):\n",
    "    sql_context = examples[\"sql_context\"]\n",
    "    sql_prompt = examples[\"sql_prompt\"]\n",
    "    sql = examples[\"sql\"]\n",
    "    sql_explanation = examples[\"sql_explanation\"]\n",
    "    messages = [\n",
    "            {\"role\": \"system\", \"content\": INSTRUCTION},\n",
    "            {\"role\": \"user\", \"content\": INPUT_EXAMPLE.format(sql_context=sql_context, sql_prompt=sql_prompt)},\n",
    "            {\"role\": \"assistant\", \"content\": OUTPUT_EXAMPLE.format(sql=sql, sql_explanation=sql_explanation)}\n",
    "        ]\n",
    "    applied_template = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    return {\"text\": applied_template}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86f26a8f-a266-457d-8f89-8f8716cda85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.map(format_example)\n",
    "val = val.map(format_example)\n",
    "test = test.map(format_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c6e50aa-d270-406a-a67d-6f86753c5025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are a specialized SQL query generator that helps users write efficient SQL queries. \\nYour role is to analyze the database schema in the `sql_context` and generate the appropriate SQL code with explanation that answers the `sql_prompt`.\\nBoth `sql_context` and `sql_prompt` are given by the user.\\n\\n### Input Example:\\nsql_context: CREATE TABLE salesperson (salesperson_id INT, name TEXT, region TEXT); INSERT INTO salesperson (salesperson_id, name, region) VALUES (1, \\'John Doe\\', \\'North\\'), (2, \\'Jane Smith\\', \\'South\\'); CREATE TABLE timber_sales (sales_id INT, salesperson_id INT, volume REAL, sale_date DATE); INSERT INTO timber_sales (sales_id, salesperson_id, volume, sale_date) VALUES (1, 1, 120, \\'2021-01-01\\'), (2, 1, 150, \\'2021-02-01\\'), (3, 2, 180, \\'2021-01-01\\');\\n\\nsql_prompt: \"What is the total volume of timber sold by each salesperson, sorted by salesperson?\"\\n\\n### Output Example:\\nSQL: SELECT salesperson_id, name, SUM(volume) as total_volume FROM timber_sales JOIN salesperson ON timber_sales.salesperson_id = salesperson.salesperson_id GROUP BY salesperson_id, name ORDER BY total_volume DESC;\\n\\nExplanation: Joins timber_sales and salesperson tables, groups sales by salesperson, calculates total volume sold by each salesperson, and orders the results by total volume in descending order.<|im_end|>\\n<|im_start|>user\\nsql_context: CREATE TABLE art_works (id INT, title VARCHAR(255), artist_name VARCHAR(255), medium VARCHAR(255)); INSERT INTO art_works (id, title, artist_name, medium) VALUES (1, \\'Starry Night\\', \\'Vincent van Gogh\\', \\'painting\\'), (2, \\'Mona Lisa\\', \\'Leonardo da Vinci\\', \\'painting\\'), (3, \\'The Persistence of Memory\\', \\'Salvador DalÃ­\\', \\'painting\\'), (4, \\'Composition VIII\\', \\'Wassily Kandinsky\\', \\'painting\\'), (5, \\'Girl with a Pearl Earring\\', \\'Johannes Vermeer\\', \\'painting\\');\\n\\nsql_prompt: Who are the top 3 artists with the most works in the \\'painting\\' medium?<|im_end|>\\n<|im_start|>assistant\\nSQL: SELECT artist_name, COUNT(*) as work_count FROM art_works WHERE medium = \\'painting\\' GROUP BY artist_name ORDER BY work_count DESC LIMIT 3;\\n\\nExplanation: This SQL query identifies the top 3 artists with the most works in the \\'painting\\' medium by using the COUNT function and GROUP BY clause on the \\'artist_name\\' column, and filtering for rows with \\'painting\\' in the \\'medium\\' column. The ORDER BY and LIMIT clauses are used to get the top 3 results.<|im_end|>\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f7ba7619-160e-4ee6-bf90-4a2e32483801",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c0e649d-3307-49d2-b1dc-b3b66eb2c5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "curated_dataset_dict = DatasetDict(\n",
    "    {\"train\": train,\n",
    "    \"val\": val,\n",
    "    \"test\": test}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f7b1e500-b56f-416f-a3b2-c9e3de42c319",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading the dataset shards:   0%|                                                              | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[Aating parquet from Arrow format:   0%|                                                       | 0/95 [00:00<?, ?ba/s]\n",
      "\u001b[Aating parquet from Arrow format:  22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                   | 21/95 [00:00<00:00, 209.90ba/s]\n",
      "\u001b[Aating parquet from Arrow format:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                         | 42/95 [00:00<00:00, 205.79ba/s]\n",
      "\u001b[Aating parquet from Arrow format:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š               | 63/95 [00:00<00:00, 207.60ba/s]\n",
      "Creating parquet from Arrow format: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [00:00<00:00, 209.01ba/s]\n",
      "\n",
      "\u001b[A%|                                                                                            | 0/1 [00:00<?, ?it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:14<00:00, 14.22s/it]\n",
      "Uploading the dataset shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:15<00:00, 15.35s/it]\n",
      "Uploading the dataset shards:   0%|                                                              | 0/1 [00:00<?, ?it/s]\n",
      "Creating parquet from Arrow format: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 222.04ba/s]\n",
      "\n",
      "\u001b[A%|                                                                                            | 0/1 [00:00<?, ?it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.35it/s]\n",
      "Uploading the dataset shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.33s/it]\n",
      "Uploading the dataset shards:   0%|                                                              | 0/1 [00:00<?, ?it/s]\n",
      "Creating parquet from Arrow format: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 223.04ba/s]\n",
      "\n",
      "\u001b[A%|                                                                                            | 0/1 [00:00<?, ?it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.41it/s]\n",
      "Uploading the dataset shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.29s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/Yihim/synthetic_text_to_sql-qwen2.5-instruct-curated/commit/92930713d1f10a6e2ade79914e84b0af2cb5a1d8', commit_message='Upload dataset', commit_description='', oid='92930713d1f10a6e2ade79914e84b0af2cb5a1d8', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/Yihim/synthetic_text_to_sql-qwen2.5-instruct-curated', endpoint='https://huggingface.co', repo_type='dataset', repo_id='Yihim/synthetic_text_to_sql-qwen2.5-instruct-curated'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curated_dataset_dict.push_to_hub(f\"{HF_USER}/synthetic_text_to_sql-qwen2.5-instruct-curated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8bbcd541-67ec-4c86-a4b0-d05837fd7fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_template = \"<|im_start|>assistant\\n\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template=response_template, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e52a4552-489a-4244-8a48-cbe77a4d4efa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_parameters = SFTConfig(\n",
    "    num_train_epochs=2,\n",
    "    output_dir=PROJECT_RUN_NAME,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=LOG_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    fp16= not is_bfloat16_supported(),\n",
    "    bf16= is_bfloat16_supported(),\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=RUN_NAME,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    save_strategy=\"steps\",\n",
    "    hub_strategy=\"end\",\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=f\"{HF_USER}/unsloth-qwen2.5-7b-instruct-text-to-sql-v1\",\n",
    "    hub_private_repo=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    seed=3407\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "60616a82-2426-48fd-b01c-655764fc97d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryCleanupCallback(TrainerCallback):\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09a6249b-bc67-4e33-8144-e5a1b9a7cbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_cleanup_callback = MemoryCleanupCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fee33624-38ef-4b12-8706-1b7ab3ab7623",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "acad2389-039a-44fc-8b0b-ddbe69091cef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing to [\"text\"] (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95000/95000 [01:02<00:00, 1524.62 examples/s]\n",
      "Tokenizing to [\"text\"] (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:11<00:00, 419.34 examples/s]\n"
     ]
    }
   ],
   "source": [
    "fine_tuning = SFTTrainer(\n",
    "    dataset_num_proc=2,\n",
    "    model=model,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=val,\n",
    "    processing_class=tokenizer,\n",
    "    args=train_parameters,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    packing=False,\n",
    "    data_collator=collator,\n",
    "    # callbacks=[early_stopping_callback, memory_cleanup_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "848fde45-bbe6-4119-b7f9-dcc7c4ad4f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9a8351e5-bf51-4050-89a9-371c126985ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 95,000 | Num Epochs = 2 | Total steps = 11,874\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 40,370,176/4,931,917,312 (0.82% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11874' max='11874' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11874/11874 22:10:11, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.979300</td>\n",
       "      <td>0.871987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.199200</td>\n",
       "      <td>0.872043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.048600</td>\n",
       "      <td>0.872973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.038100</td>\n",
       "      <td>0.871487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.221200</td>\n",
       "      <td>0.871695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.946300</td>\n",
       "      <td>0.869801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.938000</td>\n",
       "      <td>0.871059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.079700</td>\n",
       "      <td>0.791800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.639100</td>\n",
       "      <td>0.771342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.903200</td>\n",
       "      <td>0.752175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.976600</td>\n",
       "      <td>0.751881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.753400</td>\n",
       "      <td>0.736859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.728700</td>\n",
       "      <td>0.737150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.666200</td>\n",
       "      <td>0.734401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.637600</td>\n",
       "      <td>0.733274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.644700</td>\n",
       "      <td>0.732521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.708800</td>\n",
       "      <td>0.733929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.657000</td>\n",
       "      <td>0.733445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.695700</td>\n",
       "      <td>0.732333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.722200</td>\n",
       "      <td>0.732236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.717200</td>\n",
       "      <td>0.732232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.676200</td>\n",
       "      <td>0.732580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.678700</td>\n",
       "      <td>0.732521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Unsloth: Not an error, but Qwen2ForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Could not locate the best model at sql_expert--2025-03-12_11.23.04\\checkpoint-10500\\pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.\n"
     ]
    }
   ],
   "source": [
    "fine_tuning_stats = fine_tuning.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "37603d02-ad36-4f4a-8ab2-fb98bb8bc87d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=11874, training_loss=0.7826375707052805, metrics={'train_runtime': 79821.1322, 'train_samples_per_second': 2.38, 'train_steps_per_second': 0.149, 'total_flos': 4.4383198831549563e+18, 'train_loss': 0.7826375707052805})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tuning_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1f384689-002c-492d-ae99-2c8319b00bfa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to https://huggingface.co/Yihim/unsloth-qwen2.5-7b-instruct-text-to-sql-v1\n"
     ]
    }
   ],
   "source": [
    "fine_tuning.model.push_to_hub(repo_id=f\"{HF_USER}/unsloth-qwen2.5-7b-instruct-text-to-sql-v1\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3b2d623c-c86c-453d-9bc9-c37fa0df32f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\n",
      "tokenizer.json:   0%|                                                                      | 0.00/11.4M [00:00<?, ?B/s]\n",
      "tokenizer.json:   3%|â–ˆâ–Š                                                            | 328k/11.4M [00:00<00:03, 3.00MB/s]\n",
      "tokenizer.json:  10%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                       | 1.10M/11.4M [00:00<00:01, 5.47MB/s]\n",
      "tokenizer.json:  14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                    | 1.65M/11.4M [00:00<00:05, 1.82MB/s]\n",
      "tokenizer.json:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                  | 7.98M/11.4M [00:00<00:00, 12.2MB/s]\n",
      "tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11.4M/11.4M [00:03<00:00, 3.60MB/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.46s/it]\n"
     ]
    }
   ],
   "source": [
    "fine_tuning.processing_class.push_to_hub(repo_id=f\"{HF_USER}/unsloth-qwen2.5-7b-instruct-text-to-sql-v1\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e6c4309a-5f01-45c3-8a39-56b96cd480d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('unsloth-qwen2.5-7b-instruct-text-to-sql-v1\\\\tokenizer_config.json',\n",
       " 'unsloth-qwen2.5-7b-instruct-text-to-sql-v1\\\\special_tokens_map.json',\n",
       " 'unsloth-qwen2.5-7b-instruct-text-to-sql-v1\\\\vocab.json',\n",
       " 'unsloth-qwen2.5-7b-instruct-text-to-sql-v1\\\\merges.txt',\n",
       " 'unsloth-qwen2.5-7b-instruct-text-to-sql-v1\\\\added_tokens.json',\n",
       " 'unsloth-qwen2.5-7b-instruct-text-to-sql-v1\\\\tokenizer.json')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tuning.model.save_pretrained(\"unsloth-qwen2.5-7b-instruct-text-to-sql-v1\")\n",
    "fine_tuning.processing_class.save_pretrained(\"unsloth-qwen2.5-7b-instruct-text-to-sql-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "95f4c0c0-b10d-4725-a247-dd02f147ba3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model = FastLanguageModel.for_inference(fine_tuning.model)\n",
    "fine_tuned_tokenizer = fine_tuning.processing_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b71e4c3b-a7a8-49e4-a7b2-52b6817f4bfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'domain', 'domain_description', 'sql_complexity', 'sql_complexity_description', 'sql_task_type', 'sql_task_type_description', 'sql_prompt', 'sql_context', 'sql', 'sql_explanation', 'text'],\n",
       "    num_rows: 5851\n",
       "})"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c26d8228-04ff-46e6-8da3-40e35abeb264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_test_example(examples):\n",
    "    sql_context = examples[\"sql_context\"]\n",
    "    sql_prompt = examples[\"sql_prompt\"]\n",
    "    sql = examples[\"sql\"]\n",
    "    sql_explanation = examples[\"sql_explanation\"]\n",
    "    messages = [\n",
    "            {\"role\": \"system\", \"content\": INSTRUCTION},\n",
    "            {\"role\": \"user\", \"content\": INPUT_EXAMPLE.format(sql_context=sql_context, sql_prompt=sql_prompt)},\n",
    "        ]\n",
    "    return {\"messages\": messages, \"expected_output\": OUTPUT_EXAMPLE.format(sql=sql, sql_explanation=sql_explanation)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "447beee6-a8e4-4510-8901-4725ada0729c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5851/5851 [00:00<00:00, 12133.95 examples/s]\n"
     ]
    }
   ],
   "source": [
    "test = test.map(format_test_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "646ac348-956b-4cca-a306-d53bf2917593",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'You are a specialized SQL query generator that helps users write efficient SQL queries. \\nYour role is to analyze the database schema in the `sql_context` and generate the appropriate SQL code with explanation that answers the `sql_prompt`.\\nBoth `sql_context` and `sql_prompt` are given by the user.\\n\\n### Input Example:\\nsql_context: CREATE TABLE salesperson (salesperson_id INT, name TEXT, region TEXT); INSERT INTO salesperson (salesperson_id, name, region) VALUES (1, \\'John Doe\\', \\'North\\'), (2, \\'Jane Smith\\', \\'South\\'); CREATE TABLE timber_sales (sales_id INT, salesperson_id INT, volume REAL, sale_date DATE); INSERT INTO timber_sales (sales_id, salesperson_id, volume, sale_date) VALUES (1, 1, 120, \\'2021-01-01\\'), (2, 1, 150, \\'2021-02-01\\'), (3, 2, 180, \\'2021-01-01\\');\\n\\nsql_prompt: \"What is the total volume of timber sold by each salesperson, sorted by salesperson?\"\\n\\n### Output Example:\\nSQL: SELECT salesperson_id, name, SUM(volume) as total_volume FROM timber_sales JOIN salesperson ON timber_sales.salesperson_id = salesperson.salesperson_id GROUP BY salesperson_id, name ORDER BY total_volume DESC;\\n\\nExplanation: Joins timber_sales and salesperson tables, groups sales by salesperson, calculates total volume sold by each salesperson, and orders the results by total volume in descending order.', 'role': 'system'}, {'content': \"sql_context: CREATE TABLE dapp_ranking (dapp_id INT, dapp_name VARCHAR(50), dapp_category VARCHAR(30), dapp_rating DECIMAL(3,2), dapp_downloads INT, dapp_region VARCHAR(30)); INSERT INTO dapp_ranking (dapp_id, dapp_name, dapp_category, dapp_rating, dapp_downloads, dapp_region) VALUES (1, 'AsiaPacificDapp', 'Social', 4.3, 2000000, 'Asia-Pacific');\\n\\nsql_prompt: How many decentralized applications have been downloaded from the 'Asia-Pacific' region?\", 'role': 'user'}]\n"
     ]
    }
   ],
   "source": [
    "print(test[\"messages\"][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a02288fb-8fd4-46aa-9952-08953e38138e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a3f5f888-110c-4066-8a4b-c2488d1beb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_streamer = TextStreamer(skip_prompt=True, tokenizer=fine_tuned_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "2478e2d6-b0b8-466d-8487-9f1f7c0ec822",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_ids = fine_tuned_tokenizer.apply_chat_template(test[\"messages\"][10], add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "738a074c-5c92-400a-890f-55912d6b78eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL: SELECT COUNT(dapp_id) AS total_downloads FROM dapp_ranking WHERE dapp_region = 'Asia-Pacific';\n",
      "\n",
      "Explanation: The SQL query counts the number of decentralized applications (dapps) that have been downloaded from the 'Asia-Pacific' region by filtering the `dapp_ranking` table where `dapp_region` equals 'Asia-Pacific'. The result will give you the total number of dapps downloaded from this region.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "_ = fine_tuned_model.generate(input_ids, streamer=text_streamer, max_new_tokens=1024, pad_token_id=fine_tuned_tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "7460816d-e49d-4fa3-b869-c4643e3d6fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"SQL: SELECT SUM(dapp_downloads) FROM dapp_ranking WHERE dapp_region = 'Asia-Pacific';\\n\\nExplanation: The SQL query calculates the total number of downloads for all decentralized applications from the 'Asia-Pacific' region by summing the 'dapp_downloads' values for all records with the 'dapp_region' value of 'Asia-Pacific'.\""
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"expected_output\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08558330-22e2-48f8-babb-5341013ff725",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sql_expert",
   "language": "python",
   "name": "sql_expert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
